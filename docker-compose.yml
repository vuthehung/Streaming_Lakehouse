services:
  flask:
    container_name: flask
    hostname: flask
    image: flask
    networks:
      vdt_net:
    ports:
      - 5000:5000
    volumes:
      - ./flask/data:/python-docker/data
  postgres:
    image: postgres:14
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    networks:
      vdt_net:
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
  webserver:
    image: airflow
    container_name: webserver
    hostname: webserver
    command: webserver
    entrypoint: [ '/opt/airflow/script/entrypoint.sh' ]
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW_WEBSERVER_SECRET_KEY=''
      - AWS_REGION=us-east-1
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./scripts/entrypoint.sh:/opt/airflow/script/entrypoint.sh
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - airflow_scripts:/opt/airflow/scripts
      - ./spark/code:/opt/airflow/code
    ports:
      - "8282:8080"
    healthcheck:
      test: [ 'CMD-SHELL', "[ -f /opt/airflow/airflow-webserver.pid ]" ]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      vdt_net:

  scheduler:
    image: airflow
    container_name: scheduler
    hostname: scheduler
    depends_on:
      webserver:
        condition: service_healthy
    volumes:
      - ./scripts/entrypoint.sh:/opt/airflow/script/entrypoint.sh
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - airflow_scripts:/opt/airflow/scripts
      - ./spark/code:/opt/airflow/code
    environment:
      - LOAD_EX=n
      - EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW_WEBSERVER_SECRET_KEY=''
      - AWS_REGION=us-east-1
    command: bash -c "airflow db upgrade && airflow scheduler"
    networks:
      vdt_net:
  broker:
    container_name: broker
    hostname: broker
    image: confluentinc/cp-server:7.5.1
    networks:
      vdt_net:
    ports:
      - 9092:9092
    restart: always
    mem_limit: 512m
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_DEFAULT_MIN_ISR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      CLUSTER_ID: 'ZWe3nnZwTrKSM0aM2doAxQ'
    healthcheck:
      test: nc -z broker 9092 || exit -1
      start_period: 15s
      interval: 5s
      timeout: 10s
      retries: 10

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    networks:
      vdt_net:
    ports:
      - 8080:8080
    depends_on:
      broker:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: kafka-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker:29092
    mem_limit: 256m
    healthcheck:
      test: nc -z kafka-ui 8080 || exit -1
      start_period: 15s
      interval: 5s
      timeout: 10s
      retries: 10

  spark-iceberg:
    image: tabulario/spark-iceberg:3.5.0_1.4.2
    hostname: spark-iceberg
    container_name: spark-iceberg
    depends_on:
      - rest
      - minio
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_MODE=master
    ports:
      - 8888:8888
      - 8081:8080
      - 7077:7077
      - 10000:10000
      - 10001:10001
    networks:
      vdt_net:
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - iceberg_warehouse:/home/iceberg/warehouse
      - iceberg_notebooks:/home/iceberg/notebooks/notebooks
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
  spark-worker-1:
    image: tabulario/spark-iceberg:3.5.0_1.4.2
    hostname: spark-worker-1
    container_name: spark-worker-1
    depends_on:
      - spark-iceberg
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_MODE=worker
      - SPARK_MASTER_HOST=spark_iceberg
      - SPARK_MASTER_PORT=7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=3G
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "8082:8081"
    networks:
      vdt_net:
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - iceberg_warehouse:/home/iceberg/warehouse
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
  rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
    ports:
      - 8181:8181
    networks:
      vdt_net:

  # Minio cluster
  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_ACCESS_KEY=admin
      - MINIO_SECRET_KEY=password
      - MINIO_DOMAIN=minio
    ports:
      - 9001:9001
      - 9000:9000
    networks:
      vdt_net:
        aliases:
          - warehouse.minio
    volumes:
      - data:/data
    command: [ "server", "/data", "--console-address", ":9001" ]

  mc:
    image: minio/mc
    container_name: mc
    hostname: mc
    depends_on:
      - minio
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    networks:
      vdt_net:
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "
  trino:
    image: trinodb/trino:457
    container_name: trino
    hostname: trino
    ports:
      - 8383:8080
    networks:
      vdt_net:
    volumes:
      - ./trino/etc:/usr/lib/trino/etc:ro
      - ./trino/catalog:/etc/trino/catalog
  superset:
    image: superset
    container_name: superset
    hostname: superset
    environment:
      - SUPERSET_SECRET_KEY=secret
    ports:
      - 8088:8088
    networks:
      vdt_net:
    volumes:
      - superset_data:/app/superset_home
networks:
  vdt_net:

volumes:
  postgres-db-volume:
  airflow_logs:
  airflow_plugins:
  airflow_scripts:
  iceberg_warehouse:
  iceberg_notebooks:
  data:
  superset_data:
